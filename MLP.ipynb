{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Name: [Enter your name here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import any required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 – Data Acquisition\n",
    "Load the training data 'house_prices_train.csv' into a dataframe. Explore the data to get a better understanding of its structure and any data preparation steps that you need to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data and view the dimensions\n",
    "\n",
    "url      = '' #TODO: provide the url for the training data\n",
    "data     = pd.read_csv(url)\n",
    "data_dim = data.shape\n",
    "\n",
    "print ('There are {} rows and {} columns.'.format(data_dim[0], data_dim[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets view samples of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view a few observations\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use your intuition!\n",
    "At first glance is there any field that, without a doubt, will not contribute to the predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: remove/exclude the unnecessary field(s) that will not contribute towards the prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 – Data Exploration\n",
    "- Gather summary/descriptive statistics and inspect **all the fields**. This can help you to identify outliers and detect any inconsistencies\n",
    "- View the frequency of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: gather descriptive statistics to view the range of values in each field. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: show the frequency of missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State your observations about the summary statistics and missing values **(in this cell)**:\n",
    "- \n",
    "- \n",
    "- \n",
    "- \n",
    "- \n",
    "\n",
    "Note: recall that not all missing values need to be deleted, some of them can be imputed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The continuous and categorical independent variables\n",
    "List the continuous and categorical data and state any discrepancy between the number of expected records in the dataset and the `count` that is reported above. \n",
    "\n",
    "For the fields that are discussed, view `data_description.txt` which explains the range of values for each field. What does this tell you about these 'missing' values. How do you recommend addressing them? **(You do not need to demonstrate your recommendations)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The dependent variable\n",
    "Are there any discrepancies with the dependent variable? Plot a histogram showing its distribution. Is the distribution skewed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Plot the histogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Pipeline\n",
    "Based on your recommendations above, lets build a pipeline that does the following:\n",
    "- prepare the data and perform data imputation\n",
    "- transform the continuous and categorical data (scaling and encoding respectively)\n",
    "- select the useful features e.g. feature selection, *you can optionally include this in the pipeline or perform this step prior to building the pipeline*\n",
    "- build, train and evaluate the neural network using Keras.\n",
    "- perform hyper-parameter tuning using RandomSearchCV **(optional)**\n",
    "- make predictions with new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 – Data Preparation\n",
    "Here is some helpful information on [preprocessing and feature extraction pipelines in scikit-learn](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html)\n",
    "\n",
    "<span style=\"color:red\">NOTE: You can modify the cell below to suit your needs. However, ensure that the preprocessing steps that you perform is done in the data frame e.g. `data` </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute missing continuous values with the median and scale the data\n",
    "\n",
    "continuous_features  = [] #TODO: provide a list of continuous fields that will be used in the model(except the dependent variable)\n",
    "continous_transformer = Pipeline(\n",
    "    steps = [\n",
    "    ('imputer', SimpleImputer(strategy = 'median')),\n",
    "    ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "#impute the NA categorical values and encode the data\n",
    "\n",
    "categorical_features = [] #TODO: provide a list of categorical fields that will be used in the model\n",
    "categorical_transformer = Pipeline(\n",
    "    steps = [\n",
    "    ('imputer', SimpleImputer(strategy = 'constant', fill_value = 'NotApp')), #Use an alternative value to indicate NA in the dataset\n",
    "    ('onehot', OneHotEncoder(handle_unknown = 'ignore'))\n",
    "    ])\n",
    "\n",
    "data_preprocessor   = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('continious', continous_transformer, continuous_features),\n",
    "        ('categorical', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "#NOTE: the steps above will not be performed until we call `fit_transform` (in the next cell).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 – Data Transformation & Feature Selection\n",
    "Here is some helpful information on [feature selection as part of a pipeline](https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection-as-part-of-a-pipeline). If you add a feature selection algorithm to the pipeline, ensure that it supports regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_pipeline  = Pipeline(steps=[('preprocessor', data_preprocessor), #This performs the data preparation steps in the cell above\n",
    "                     ('feature_selection',  #TODO: identify a feature selection algorithm or exclude this line if you have previously performed feature selection on the data.\n",
    "                                                          ), \n",
    "                    ])\n",
    "\n",
    "transformed_data    = data_prep_pipeline.fit_transform(data.iloc[:, :-1], data['SalePrice']) #transform the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 – Building the Model\n",
    "#### Build the neural network using Keras\n",
    "Build a feed forward neural network with: an input layer, hidden layers and one output layer. \n",
    "\n",
    "Note: you are required to provide a suitable [optimizer](https://keras.io/api/optimizers/) and [loss function](https://keras.io/api/losses/) for the regression task. Optimizers include: 'Adam', 'SGD' and RMSprop. Loss functions include: 'mean_squared_error', 'mean_squared_logarithmic_error', 'mean_absolute_error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = transformed_data #this is the transformed data from the pipeline\n",
    "y = data['SalePrice'] #this is the output\n",
    "\n",
    "#Build a sequential model with at least three dense layers (you can add more layers as needed)\n",
    "#Note: you can also add this keras model to the data preprocessing pipeline but we can skip that step for now.\n",
    "ffnn_model = Sequential()\n",
    "ffnn_model.add(Dense(40, activation='relu', input_shape=(X.shape[1],))) #X.shape[1] is the number of selected features \n",
    "\n",
    "#TODO: Add the first hidden layer with a suitable number of units/neurons and the 'relu' activation function\n",
    "#TODO: Add the second hidden layer with a suitable number of units/neurons and the 'relu' activation function\n",
    "\n",
    "#TODO: Add the output layer\n",
    "\n",
    "ffnn_model.compile(optimizer= , #TODO: state the optimize\n",
    "                   loss= ,      #TODO: state the loss function\n",
    "                   metrics=     #TODO: state the metric\n",
    "                  )\n",
    "\n",
    "ffnn_history = ffnn_model.fit(X, y, \n",
    "                              validation_split= , #TODO: state the validation split\n",
    "                              epochs= , #TODO: state the number of epochs (you may need to run the model a few times to find a suitable value)\n",
    "                              batch_size= , #TODO: state the number of observations to use in each batch\n",
    "                              verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training and validation loss\n",
    "plt.plot(ffnn_history.history['loss'], 'b', ffnn_history.history['val_loss'], 'orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use the neural network to make predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Load the data from `house_prices_test.csv`\n",
    "test_data_url = ''\n",
    "test_data = pd.read_csv(test_data_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the test data using the pipeline\n",
    "This will impute any missing values and scale/encode the fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_test_data = data_prep_pipeline.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the neural network to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ffnn_model.predict( #TODO: provide the preprocessed test data (above)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Display samples of the predictions from your model and summarize your thoughts on the model's performance, the training process and its ability to generalize with new data. What are your recommendations to improve the model in the future?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
